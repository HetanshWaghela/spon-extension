{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPON Results Visualization\n",
    "\n",
    "This notebook helps you analyze and visualize experimental results from SPON allocation sweeps.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to load and parse experimental results\n",
    "2. How to plot Pareto frontiers (parameter-performance trade-offs)\n",
    "3. How to compare configurations statistically\n",
    "4. How to generate publication-ready figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "from src.result_manager import ExperimentManager\n",
    "from src.evaluation import compute_pareto_frontier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experimental Results\n",
    "\n",
    "Specify the path to your experiment results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your results directory\n",
    "RESULTS_DIR = Path(\"../results/allocation_sweep\")\n",
    "\n",
    "if RESULTS_DIR.exists():\n",
    "    exp_manager = ExperimentManager(\"allocation_sweep\", str(RESULTS_DIR.parent))\n",
    "    runs = exp_manager.list_runs()\n",
    "    print(f\"Found {len(runs)} runs:\")\n",
    "    for run in runs:\n",
    "        print(f\"  - {run}\")\n",
    "else:\n",
    "    print(f\"Results directory not found: {RESULTS_DIR}\")\n",
    "    print(\"Run the experiment first, or we'll use demo data below.\")\n",
    "    runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results (or create demo data)\n",
    "if runs:\n",
    "    # Load real results\n",
    "    all_results = []\n",
    "    for run_id in runs:\n",
    "        run_data = exp_manager.load_run(run_id)\n",
    "        if run_data and \"results\" in run_data:\n",
    "            all_results.extend(run_data[\"results\"])\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(f\"Loaded {len(df)} result rows\")\n",
    "else:\n",
    "    # Demo data (typical SPON results)\n",
    "    demo_data = [\n",
    "        {\"config_name\": \"BASELINE-TEAL\", \"sparsity\": 0.5, \"perplexity\": 18.5, \"relative_params\": 0.0},\n",
    "        {\"config_name\": \"UNIF-ALL\", \"sparsity\": 0.5, \"perplexity\": 15.2, \"relative_params\": 1.0},\n",
    "        {\"config_name\": \"TOP-25\", \"sparsity\": 0.5, \"perplexity\": 17.1, \"relative_params\": 0.25},\n",
    "        {\"config_name\": \"TOP-50\", \"sparsity\": 0.5, \"perplexity\": 15.8, \"relative_params\": 0.50},\n",
    "        {\"config_name\": \"TOP-75\", \"sparsity\": 0.5, \"perplexity\": 15.4, \"relative_params\": 0.75},\n",
    "        {\"config_name\": \"BOTTOM-50\", \"sparsity\": 0.5, \"perplexity\": 16.9, \"relative_params\": 0.50},\n",
    "        {\"config_name\": \"BASELINE-TEAL\", \"sparsity\": 0.6, \"perplexity\": 22.1, \"relative_params\": 0.0},\n",
    "        {\"config_name\": \"UNIF-ALL\", \"sparsity\": 0.6, \"perplexity\": 17.8, \"relative_params\": 1.0},\n",
    "        {\"config_name\": \"TOP-50\", \"sparsity\": 0.6, \"perplexity\": 18.9, \"relative_params\": 0.50},\n",
    "    ]\n",
    "    df = pd.DataFrame(demo_data)\n",
    "    print(\"Using demo data (run experiment to see real results)\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pareto Frontier Analysis\n",
    "\n",
    "The **Pareto frontier** shows optimal trade-offs between parameter count and performance.\n",
    "\n",
    "A configuration is Pareto-optimal if no other configuration achieves both:\n",
    "- Fewer parameters AND\n",
    "- Lower perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_frontier(df, sparsity=0.5, ax=None):\n",
    "    \"\"\"Plot Pareto frontier for a given sparsity level.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Filter by sparsity\n",
    "    subset = df[df['sparsity'] == sparsity].copy()\n",
    "    \n",
    "    if len(subset) == 0:\n",
    "        print(f\"No data for sparsity={sparsity}\")\n",
    "        return\n",
    "    \n",
    "    # Compute Pareto frontier\n",
    "    pareto = compute_pareto_frontier(\n",
    "        subset.to_dict('records'),\n",
    "        x_key='relative_params',\n",
    "        y_key='perplexity'\n",
    "    )\n",
    "    pareto_df = pd.DataFrame(pareto)\n",
    "    \n",
    "    # Plot all points\n",
    "    colors = {'BASELINE-TEAL': 'red', 'UNIF-ALL': 'blue'}\n",
    "    for _, row in subset.iterrows():\n",
    "        color = colors.get(row['config_name'], 'gray')\n",
    "        is_pareto = row['config_name'] in pareto_df['config_name'].values if len(pareto_df) > 0 else False\n",
    "        marker = '*' if is_pareto else 'o'\n",
    "        size = 200 if is_pareto else 100\n",
    "        ax.scatter(\n",
    "            row['relative_params'], row['perplexity'],\n",
    "            c=color, s=size, marker=marker, alpha=0.8,\n",
    "            edgecolors='black', linewidths=1\n",
    "        )\n",
    "        ax.annotate(\n",
    "            row['config_name'], \n",
    "            (row['relative_params'], row['perplexity']),\n",
    "            xytext=(5, 5), textcoords='offset points', fontsize=9\n",
    "        )\n",
    "    \n",
    "    # Plot Pareto frontier line\n",
    "    if len(pareto_df) > 1:\n",
    "        pareto_sorted = pareto_df.sort_values('relative_params')\n",
    "        ax.plot(\n",
    "            pareto_sorted['relative_params'], pareto_sorted['perplexity'],\n",
    "            'g--', linewidth=2, label='Pareto frontier'\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Relative Parameters (vs UNIF-ALL)', fontsize=12)\n",
    "    ax.set_ylabel('Perplexity', fontsize=12)\n",
    "    ax.set_title(f'SPON Allocation Trade-offs (Sparsity={sparsity:.0%})', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return pareto_df\n",
    "\n",
    "# Plot for 50% sparsity\n",
    "pareto = plot_pareto_frontier(df, sparsity=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if pareto is not None and len(pareto) > 0:\n",
    "    print(\"\\nPareto-optimal configurations:\")\n",
    "    print(pareto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Comparison\n",
    "\n",
    "Compare all configurations against the TEAL baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_config_comparison(df, sparsity=0.5):\n",
    "    \"\"\"Bar chart comparing configurations.\"\"\"\n",
    "    subset = df[df['sparsity'] == sparsity].copy()\n",
    "    \n",
    "    # Get TEAL baseline\n",
    "    teal_ppl = subset[subset['config_name'] == 'BASELINE-TEAL']['perplexity'].values\n",
    "    if len(teal_ppl) == 0:\n",
    "        print(\"No TEAL baseline found\")\n",
    "        return\n",
    "    teal_ppl = teal_ppl[0]\n",
    "    \n",
    "    # Compute improvement\n",
    "    subset['improvement'] = (teal_ppl - subset['perplexity']) / teal_ppl * 100\n",
    "    subset = subset.sort_values('improvement', ascending=True)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Perplexity\n",
    "    colors = ['green' if x > 0 else 'red' for x in subset['improvement']]\n",
    "    bars = ax1.barh(subset['config_name'], subset['perplexity'], color=colors, alpha=0.7)\n",
    "    ax1.axvline(x=teal_ppl, color='red', linestyle='--', label=f'TEAL baseline ({teal_ppl:.1f})')\n",
    "    ax1.set_xlabel('Perplexity', fontsize=12)\n",
    "    ax1.set_title(f'Perplexity by Configuration (Sparsity={sparsity:.0%})', fontsize=14)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Improvement\n",
    "    colors = ['green' if x > 0 else 'red' for x in subset['improvement']]\n",
    "    ax2.barh(subset['config_name'], subset['improvement'], color=colors, alpha=0.7)\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.set_xlabel('PPL Improvement vs TEAL (%)', fontsize=12)\n",
    "    ax2.set_title('Relative Improvement', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return subset[['config_name', 'perplexity', 'relative_params', 'improvement']]\n",
    "\n",
    "comparison = plot_config_comparison(df, sparsity=0.5)\n",
    "if comparison is not None:\n",
    "    print(\"\\nDetailed comparison:\")\n",
    "    print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sparsity Sensitivity Analysis\n",
    "\n",
    "How does performance change across sparsity levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sparsity_sensitivity(df):\n",
    "    \"\"\"Plot perplexity vs sparsity for each configuration.\"\"\"\n",
    "    configs = df['config_name'].unique()\n",
    "    sparsities = sorted(df['sparsity'].unique())\n",
    "    \n",
    "    if len(sparsities) < 2:\n",
    "        print(\"Need at least 2 sparsity levels for sensitivity analysis\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for config in configs:\n",
    "        subset = df[df['config_name'] == config].sort_values('sparsity')\n",
    "        if len(subset) >= 2:\n",
    "            linestyle = '--' if config == 'BASELINE-TEAL' else '-'\n",
    "            ax.plot(\n",
    "                subset['sparsity'] * 100, subset['perplexity'],\n",
    "                marker='o', linestyle=linestyle, linewidth=2, markersize=8,\n",
    "                label=config\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Sparsity (%)', fontsize=12)\n",
    "    ax.set_ylabel('Perplexity', fontsize=12)\n",
    "    ax.set_title('SPON Performance vs Sparsity Level', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sparsity_sensitivity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Efficiency Analysis\n",
    "\n",
    "Which configurations give the best \"bang for your buck\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_efficiency(df, sparsity=0.5):\n",
    "    \"\"\"Compute efficiency = improvement per parameter.\"\"\"\n",
    "    subset = df[df['sparsity'] == sparsity].copy()\n",
    "    \n",
    "    teal_ppl = subset[subset['config_name'] == 'BASELINE-TEAL']['perplexity'].values\n",
    "    if len(teal_ppl) == 0:\n",
    "        return None\n",
    "    teal_ppl = teal_ppl[0]\n",
    "    \n",
    "    # Filter out TEAL baseline\n",
    "    subset = subset[subset['relative_params'] > 0].copy()\n",
    "    \n",
    "    # Compute improvement and efficiency\n",
    "    subset['improvement'] = (teal_ppl - subset['perplexity']) / teal_ppl * 100\n",
    "    subset['efficiency'] = subset['improvement'] / subset['relative_params']\n",
    "    \n",
    "    subset = subset.sort_values('efficiency', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(subset)))\n",
    "    bars = ax.barh(subset['config_name'], subset['efficiency'], color=colors, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Efficiency (% improvement / relative params)', fontsize=12)\n",
    "    ax.set_title(f'Parameter Efficiency (Sparsity={sparsity:.0%})', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, eff in zip(bars, subset['efficiency']):\n",
    "        ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                f'{eff:.1f}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return subset[['config_name', 'improvement', 'relative_params', 'efficiency']]\n",
    "\n",
    "eff_df = compute_efficiency(df, sparsity=0.5)\n",
    "if eff_df is not None:\n",
    "    print(\"\\nEfficiency ranking:\")\n",
    "    print(eff_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Publication-Ready Figures\n",
    "\n",
    "Save high-quality figures for papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_publication_figure(df, output_path, sparsity=0.5):\n",
    "    \"\"\"Export publication-ready Pareto frontier figure.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Use publication-ready style\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'legend.fontsize': 11,\n",
    "    })\n",
    "    \n",
    "    plot_pareto_frontier(df, sparsity=sparsity, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved to {output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Export\n",
    "output_dir = Path(\"../results/figures\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "export_publication_figure(df, output_dir / \"pareto_frontier.png\", sparsity=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\" * 60)\n",
    "print(\"SPON ALLOCATION EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sparsity in sorted(df['sparsity'].unique()):\n",
    "    print(f\"\\n--- Sparsity: {sparsity:.0%} ---\")\n",
    "    subset = df[df['sparsity'] == sparsity]\n",
    "    \n",
    "    teal = subset[subset['config_name'] == 'BASELINE-TEAL']['perplexity'].values\n",
    "    best = subset.loc[subset['perplexity'].idxmin()]\n",
    "    \n",
    "    if len(teal) > 0:\n",
    "        print(f\"  TEAL baseline:  {teal[0]:.2f} PPL\")\n",
    "    print(f\"  Best config:    {best['config_name']} ({best['perplexity']:.2f} PPL)\")\n",
    "    if len(teal) > 0:\n",
    "        improvement = (teal[0] - best['perplexity']) / teal[0] * 100\n",
    "        print(f\"  Improvement:    {improvement:.1f}%\")\n",
    "        print(f\"  Params used:    {best['relative_params']:.0%} of UNIF-ALL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
