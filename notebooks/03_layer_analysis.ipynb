{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPON Layer Analysis\n",
    "\n",
    "This notebook provides deep analysis of layer-wise SPON behavior.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Which layers benefit most from SPON biases\n",
    "2. How to visualize SPON bias magnitudes across layers\n",
    "3. Hidden state shift analysis (L2 distance between dense and sparse)\n",
    "4. Layer sensitivity / importance ranking\n",
    "\n",
    "**Key Research Questions:**\n",
    "- Do early layers (near embeddings) need more SPON than later layers?\n",
    "- Can we identify \"cornerstone\" layers where SPON is critical?\n",
    "- What do the learned SPON biases encode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Trained SPON Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "num_layers = len(model.model.layers)\n",
    "print(f\"Loaded model with {num_layers} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for saved SPON biases\n",
    "checkpoint_dir = Path(\"../results/allocation_sweep/runs\")\n",
    "spon_biases = None\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    # Find most recent checkpoint\n",
    "    checkpoints = list(checkpoint_dir.glob(\"*/checkpoints/*.pt\"))\n",
    "    if checkpoints:\n",
    "        latest = sorted(checkpoints)[-1]\n",
    "        print(f\"Loading SPON biases from: {latest}\")\n",
    "        checkpoint = torch.load(latest, map_location=\"cuda\")\n",
    "        spon_biases = checkpoint.get(\"biases\", {})\n",
    "        print(f\"Loaded {len(spon_biases)} bias tensors\")\n",
    "    else:\n",
    "        print(\"No checkpoints found - will train fresh biases\")\n",
    "else:\n",
    "    print(\"No results directory - will train fresh biases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no saved biases, train them\n",
    "if spon_biases is None or len(spon_biases) == 0:\n",
    "    from src.allocation import SPONConfig\n",
    "    from src.spon_trainer import SPONTrainer, TrainingArgs, create_calibration_dataloader\n",
    "    \n",
    "    # Train on ALL layers for analysis\n",
    "    config = SPONConfig(\n",
    "        name=\"UNIF-ALL\",\n",
    "        layer_mask=list(range(num_layers)),\n",
    "        modules=[\"down_proj\"]\n",
    "    )\n",
    "    \n",
    "    args = TrainingArgs(\n",
    "        epochs=3,\n",
    "        learning_rate=1e-4,\n",
    "        batch_size=4,\n",
    "        block_size=64,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    print(\"Creating calibration data...\")\n",
    "    dataloader = create_calibration_dataloader(\n",
    "        tokenizer, block_size=64, batch_size=4, num_samples=256\n",
    "    )\n",
    "    \n",
    "    print(\"Training SPON biases on all layers...\")\n",
    "    trainer = SPONTrainer(model, config, sparsity=0.5, args=args)\n",
    "    trainer.train(dataloader)\n",
    "    spon_biases = trainer.get_spon_biases()\n",
    "    print(f\"Trained {len(spon_biases)} bias tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SPON Bias Magnitude Analysis\n",
    "\n",
    "Larger bias magnitudes suggest the layer needs more compensation for sparsification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bias_magnitudes(spon_biases):\n",
    "    \"\"\"Analyze SPON bias magnitudes per layer.\"\"\"\n",
    "    layer_stats = []\n",
    "    \n",
    "    for key, bias in sorted(spon_biases.items()):\n",
    "        # Extract layer index\n",
    "        parts = key.split('_')\n",
    "        layer_idx = int(parts[1])\n",
    "        module = '_'.join(parts[2:])\n",
    "        \n",
    "        bias_np = bias.float().cpu().numpy()\n",
    "        \n",
    "        layer_stats.append({\n",
    "            'layer': layer_idx,\n",
    "            'module': module,\n",
    "            'l2_norm': np.linalg.norm(bias_np),\n",
    "            'mean': np.mean(bias_np),\n",
    "            'std': np.std(bias_np),\n",
    "            'max_abs': np.max(np.abs(bias_np)),\n",
    "            'sparsity': np.mean(np.abs(bias_np) < 1e-6),\n",
    "            'dim': len(bias_np)\n",
    "        })\n",
    "    \n",
    "    return layer_stats\n",
    "\n",
    "stats = analyze_bias_magnitudes(spon_biases)\n",
    "\n",
    "# Plot L2 norms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "layers = [s['layer'] for s in stats]\n",
    "\n",
    "# L2 Norm\n",
    "ax = axes[0, 0]\n",
    "ax.bar(layers, [s['l2_norm'] for s in stats], color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('L2 Norm')\n",
    "ax.set_title('SPON Bias L2 Norm by Layer')\n",
    "ax.axhline(y=np.mean([s['l2_norm'] for s in stats]), color='red', linestyle='--', label='Mean')\n",
    "ax.legend()\n",
    "\n",
    "# Mean value\n",
    "ax = axes[0, 1]\n",
    "colors = ['green' if s['mean'] > 0 else 'red' for s in stats]\n",
    "ax.bar(layers, [s['mean'] for s in stats], color=colors, alpha=0.8)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Mean Bias Value')\n",
    "ax.set_title('Mean SPON Bias by Layer')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Std deviation\n",
    "ax = axes[1, 0]\n",
    "ax.bar(layers, [s['std'] for s in stats], color='orange', alpha=0.8)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Std Deviation')\n",
    "ax.set_title('SPON Bias Variability by Layer')\n",
    "\n",
    "# Max absolute value\n",
    "ax = axes[1, 1]\n",
    "ax.bar(layers, [s['max_abs'] for s in stats], color='purple', alpha=0.8)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Max |Bias|')\n",
    "ax.set_title('Maximum Absolute Bias by Layer')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('SPON Bias Analysis Across Layers', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hidden State Shift Analysis\n",
    "\n",
    "Measure how much sparsification changes hidden states at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spon_trainer import create_calibration_dataloader\n",
    "from src.evaluation import compute_hidden_state_shift\n",
    "\n",
    "# Create evaluation data\n",
    "eval_dataloader = create_calibration_dataloader(\n",
    "    tokenizer, block_size=64, batch_size=4, num_samples=64\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Compute shifts WITHOUT SPON\n",
    "print(\"Computing hidden state shifts (TEAL only)...\")\n",
    "shifts_no_spon = compute_hidden_state_shift(\n",
    "    model, eval_dataloader, sparsity=0.5,\n",
    "    spon_biases=None, device=device\n",
    ")\n",
    "\n",
    "# Compute shifts WITH SPON\n",
    "print(\"Computing hidden state shifts (TEAL + SPON)...\")\n",
    "shifts_with_spon = compute_hidden_state_shift(\n",
    "    model, eval_dataloader, sparsity=0.5,\n",
    "    spon_biases=spon_biases, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and plot\n",
    "def parse_shifts(shifts_dict):\n",
    "    \"\"\"Parse shift dict into layer-indexed array.\"\"\"\n",
    "    layer_shifts = {}\n",
    "    for key, shift in shifts_dict.items():\n",
    "        layer_idx = int(key.split('_')[1])\n",
    "        layer_shifts[layer_idx] = shift\n",
    "    return layer_shifts\n",
    "\n",
    "no_spon = parse_shifts(shifts_no_spon)\n",
    "with_spon = parse_shifts(shifts_with_spon)\n",
    "\n",
    "layers = sorted(no_spon.keys())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(layers))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, [no_spon[l] for l in layers], width, \n",
    "               label='TEAL only', color='red', alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, [with_spon[l] for l in layers], width,\n",
    "               label='TEAL + SPON', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Layer Index', fontsize=12)\n",
    "ax.set_ylabel('L2 Shift (vs Dense)', fontsize=12)\n",
    "ax.set_title('Hidden State Drift from Sparsification', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(layers)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute reduction\n",
    "print(\"\\nSPON Drift Reduction by Layer:\")\n",
    "print(\"-\" * 40)\n",
    "for l in layers:\n",
    "    reduction = (no_spon[l] - with_spon[l]) / no_spon[l] * 100\n",
    "    print(f\"Layer {l:2d}: {reduction:5.1f}% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer Importance Ranking\n",
    "\n",
    "Which layers contribute most to SPON's effectiveness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute importance based on drift reduction\n",
    "importance = {}\n",
    "for l in layers:\n",
    "    # Importance = how much this layer's SPON reduces drift\n",
    "    importance[l] = no_spon[l] - with_spon[l]\n",
    "\n",
    "# Normalize\n",
    "total = sum(importance.values())\n",
    "importance_pct = {l: v/total * 100 for l, v in importance.items()}\n",
    "\n",
    "# Sort by importance\n",
    "sorted_layers = sorted(importance_pct.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "ax1.bar([str(l) for l, _ in sorted_layers], [v for _, v in sorted_layers],\n",
    "        color=plt.cm.RdYlGn(np.linspace(0.8, 0.2, len(sorted_layers))))\n",
    "ax1.set_xlabel('Layer Index (sorted by importance)')\n",
    "ax1.set_ylabel('Importance (%)')\n",
    "ax1.set_title('Layer Importance for SPON')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cumulative importance\n",
    "cumulative = np.cumsum([v for _, v in sorted_layers])\n",
    "ax2.plot(range(1, len(cumulative)+1), cumulative, 'b-o', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=80, color='red', linestyle='--', label='80% threshold')\n",
    "ax2.axhline(y=95, color='orange', linestyle='--', label='95% threshold')\n",
    "ax2.set_xlabel('Number of Layers (most important first)')\n",
    "ax2.set_ylabel('Cumulative Importance (%)')\n",
    "ax2.set_title('Cumulative Layer Importance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find minimum layers for thresholds\n",
    "for threshold in [80, 90, 95]:\n",
    "    n_layers = np.searchsorted(cumulative, threshold) + 1\n",
    "    print(f\"{threshold}% importance achieved with top {n_layers} layers ({n_layers/num_layers*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bias Distribution Visualization\n",
    "\n",
    "What do the learned biases look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias distributions for key layers\n",
    "top_3_layers = [l for l, _ in sorted_layers[:3]]\n",
    "bottom_3_layers = [l for l, _ in sorted_layers[-3:]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for idx, layer in enumerate(top_3_layers):\n",
    "    key = f\"layer_{layer}_down_proj\"\n",
    "    if key in spon_biases:\n",
    "        bias = spon_biases[key].float().cpu().numpy()\n",
    "        ax = axes[0, idx]\n",
    "        ax.hist(bias, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "        ax.axvline(x=0, color='red', linestyle='--')\n",
    "        ax.set_title(f'Layer {layer} (High Importance)\\nμ={np.mean(bias):.4f}, σ={np.std(bias):.4f}')\n",
    "        ax.set_xlabel('Bias Value')\n",
    "        ax.set_ylabel('Count')\n",
    "\n",
    "for idx, layer in enumerate(bottom_3_layers):\n",
    "    key = f\"layer_{layer}_down_proj\"\n",
    "    if key in spon_biases:\n",
    "        bias = spon_biases[key].float().cpu().numpy()\n",
    "        ax = axes[1, idx]\n",
    "        ax.hist(bias, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "        ax.axvline(x=0, color='red', linestyle='--')\n",
    "        ax.set_title(f'Layer {layer} (Low Importance)\\nμ={np.mean(bias):.4f}, σ={np.std(bias):.4f}')\n",
    "        ax.set_xlabel('Bias Value')\n",
    "        ax.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('SPON Bias Distributions: High vs Low Importance Layers', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heatmap: Bias Values Across Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of bias magnitudes\n",
    "# Subsample dimensions for visualization\n",
    "max_dims = 100  # Show first 100 dimensions\n",
    "\n",
    "bias_matrix = []\n",
    "for layer in range(num_layers):\n",
    "    key = f\"layer_{layer}_down_proj\"\n",
    "    if key in spon_biases:\n",
    "        bias = spon_biases[key].float().cpu().numpy()[:max_dims]\n",
    "        # Pad if needed\n",
    "        if len(bias) < max_dims:\n",
    "            bias = np.pad(bias, (0, max_dims - len(bias)))\n",
    "        bias_matrix.append(bias)\n",
    "\n",
    "bias_matrix = np.array(bias_matrix)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Center colormap at 0\n",
    "vmax = np.max(np.abs(bias_matrix))\n",
    "im = ax.imshow(bias_matrix, aspect='auto', cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "\n",
    "ax.set_xlabel('Hidden Dimension (first 100)', fontsize=12)\n",
    "ax.set_ylabel('Layer Index', fontsize=12)\n",
    "ax.set_title('SPON Bias Values Across Layers and Dimensions', fontsize=14)\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Bias Value', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LAYER ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Total layers: {num_layers}\")\n",
    "print(f\"Sparsity: 50%\")\n",
    "\n",
    "print(\"\\n--- Most Important Layers ---\")\n",
    "for i, (layer, imp) in enumerate(sorted_layers[:5]):\n",
    "    print(f\"  {i+1}. Layer {layer}: {imp:.1f}% importance\")\n",
    "\n",
    "print(\"\\n--- Least Important Layers ---\")\n",
    "for i, (layer, imp) in enumerate(sorted_layers[-3:]):\n",
    "    print(f\"  Layer {layer}: {imp:.1f}% importance\")\n",
    "\n",
    "print(\"\\n--- Efficiency Recommendations ---\")\n",
    "for threshold in [80, 90, 95]:\n",
    "    n = np.searchsorted(cumulative, threshold) + 1\n",
    "    layers_to_use = [l for l, _ in sorted_layers[:n]]\n",
    "    print(f\"  For {threshold}% effectiveness: Use layers {sorted(layers_to_use)}\")\n",
    "    print(f\"    -> {n}/{num_layers} layers = {(1-n/num_layers)*100:.0f}% parameter savings\")\n",
    "\n",
    "print(\"\\n--- Key Insights ---\")\n",
    "early_importance = sum(imp for l, imp in importance_pct.items() if l < num_layers//4)\n",
    "middle_importance = sum(imp for l, imp in importance_pct.items() if num_layers//4 <= l < 3*num_layers//4)\n",
    "late_importance = sum(imp for l, imp in importance_pct.items() if l >= 3*num_layers//4)\n",
    "\n",
    "print(f\"  Early layers (0-{num_layers//4-1}): {early_importance:.1f}% importance\")\n",
    "print(f\"  Middle layers ({num_layers//4}-{3*num_layers//4-1}): {middle_importance:.1f}% importance\")\n",
    "print(f\"  Late layers ({3*num_layers//4}-{num_layers-1}): {late_importance:.1f}% importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
