{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPON Extensions: Quick Start Demo\n",
    "\n",
    "This notebook provides an interactive walkthrough of the SPON (Spontaneous Neuron Activation) extension codebase.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How magnitude-based sparsification works (TEAL-style)\n",
    "2. How SPON biases compensate for information loss\n",
    "3. How to train SPON biases via KL divergence\n",
    "4. How to evaluate and compare configurations\n",
    "\n",
    "**Prerequisites:**\n",
    "- GPU with at least 8GB VRAM (for LLaMA-3.2-1B)\n",
    "- HuggingFace account with access to LLaMA models\n",
    "- Run `pip install -r requirements.txt` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Magnitude-Based Sparsification\n",
    "\n",
    "TEAL (Training-free Activation Sparsification) zeros out small activations to reduce computation.\n",
    "\n",
    "**Key idea:** Keep only the top-k% activations by absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sparse_forward import magnitude_sparsify, compute_sparsity_stats\n",
    "\n",
    "# Create sample activations (simulating a hidden state)\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, 10, 64)  # (batch=1, seq_len=10, hidden_dim=64)\n",
    "\n",
    "# Apply different sparsity levels\n",
    "sparsity_levels = [0.0, 0.25, 0.5, 0.75]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for ax, sparsity in zip(axes, sparsity_levels):\n",
    "    x_sparse = magnitude_sparsify(x, sparsity)\n",
    "    stats = compute_sparsity_stats(x_sparse)\n",
    "    \n",
    "    # Visualize first token's activations\n",
    "    ax.bar(range(64), x_sparse[0, 0].numpy(), alpha=0.7)\n",
    "    ax.set_title(f\"Sparsity={sparsity:.0%}\\n{stats['sparsity']:.1%} zeros\")\n",
    "    ax.set_xlabel(\"Hidden dimension\")\n",
    "    ax.set_ylabel(\"Activation value\")\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Effect of Magnitude-Based Sparsification\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem: Information Loss\n",
    "\n",
    "When we zero activations, we lose information. This degrades the model's output.\n",
    "\n",
    "Let's quantify this with a simple simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a linear layer\n",
    "torch.manual_seed(42)\n",
    "W = torch.randn(64, 32)  # Weight matrix (hidden_dim -> output_dim)\n",
    "x = torch.randn(1, 10, 64)  # Input activations\n",
    "\n",
    "# Dense output (ground truth)\n",
    "y_dense = x @ W\n",
    "\n",
    "# Sparse outputs at different sparsity levels\n",
    "errors = []\n",
    "sparsities = np.linspace(0, 0.9, 20)\n",
    "\n",
    "for s in sparsities:\n",
    "    x_sparse = magnitude_sparsify(x, s)\n",
    "    y_sparse = x_sparse @ W\n",
    "    \n",
    "    # L2 error\n",
    "    error = torch.norm(y_dense - y_sparse, p=2, dim=-1).mean().item()\n",
    "    errors.append(error)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sparsities * 100, errors, 'b-o', linewidth=2, markersize=6)\n",
    "plt.xlabel(\"Sparsity (% zeros)\", fontsize=12)\n",
    "plt.ylabel(\"L2 Error (vs dense output)\", fontsize=12)\n",
    "plt.title(\"Information Loss from Sparsification\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='g', linestyle='--', label='Dense (target)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At 50% sparsity: L2 error = {errors[10]:.3f}\")\n",
    "print(f\"At 75% sparsity: L2 error = {errors[15]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Solution: SPON Biases\n",
    "\n",
    "SPON adds **learned constant biases** to compensate for information loss.\n",
    "\n",
    "**Key insight:** The average effect of zeroing activations can be approximated by a constant bias!\n",
    "\n",
    "$$Y_{sparse} = W \\cdot S(X) + b_{spon}$$\n",
    "\n",
    "Where $b_{spon}$ is trained to minimize: $\\text{KL}(P_{dense} || P_{sparse+spon})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demonstration: optimal bias = mean of lost activation contribution\n",
    "sparsity = 0.5\n",
    "x_sparse = magnitude_sparsify(x, sparsity)\n",
    "x_lost = x - x_sparse  # What we zeroed out\n",
    "\n",
    "# The \"lost\" contribution to output\n",
    "y_lost = x_lost @ W\n",
    "\n",
    "# Optimal SPON bias (input-independent approximation)\n",
    "# This is what SPON training learns!\n",
    "b_spon_optimal = y_lost.mean(dim=(0, 1))  # Average over batch and sequence\n",
    "\n",
    "# Compare errors\n",
    "y_dense = x @ W\n",
    "y_sparse = x_sparse @ W\n",
    "y_sparse_spon = y_sparse + b_spon_optimal\n",
    "\n",
    "error_sparse = torch.norm(y_dense - y_sparse, p=2, dim=-1).mean().item()\n",
    "error_spon = torch.norm(y_dense - y_sparse_spon, p=2, dim=-1).mean().item()\n",
    "\n",
    "print(f\"L2 Error without SPON: {error_sparse:.3f}\")\n",
    "print(f\"L2 Error with SPON:    {error_spon:.3f}\")\n",
    "print(f\"Error reduction:       {(1 - error_spon/error_sparse)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Real Model\n",
    "\n",
    "Now let's work with an actual LLM. We'll use LLaMA-3.2-1B for fast iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"  Layers: {len(model.model.layers)}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying Sparsification with Hooks\n",
    "\n",
    "We use **forward hooks** to apply sparsification non-destructively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sparse_forward import register_sparsification_hooks, remove_hooks\n",
    "\n",
    "# Test input\n",
    "text = \"The capital of France is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Dense forward\n",
    "with torch.no_grad():\n",
    "    dense_output = model(**inputs)\n",
    "    dense_next_token = dense_output.logits[0, -1].argmax()\n",
    "    print(f\"Dense prediction: {tokenizer.decode(dense_next_token)}\")\n",
    "\n",
    "# Sparse forward (50% sparsity)\n",
    "hooks = register_sparsification_hooks(model, sparsity=0.5, target_modules=[\"down_proj\"])\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        sparse_output = model(**inputs)\n",
    "        sparse_next_token = sparse_output.logits[0, -1].argmax()\n",
    "        print(f\"Sparse prediction (50%): {tokenizer.decode(sparse_next_token)}\")\n",
    "finally:\n",
    "    remove_hooks(hooks)\n",
    "\n",
    "# Sparse forward (75% sparsity)\n",
    "hooks = register_sparsification_hooks(model, sparsity=0.75, target_modules=[\"down_proj\"])\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        sparse_output = model(**inputs)\n",
    "        sparse_next_token = sparse_output.logits[0, -1].argmax()\n",
    "        print(f\"Sparse prediction (75%): {tokenizer.decode(sparse_next_token)}\")\n",
    "finally:\n",
    "    remove_hooks(hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training SPON Biases\n",
    "\n",
    "Now let's train SPON biases to compensate for sparsification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.allocation import SPONConfig\n",
    "from src.spon_trainer import SPONTrainer, TrainingArgs, create_calibration_dataloader\n",
    "\n",
    "# Create a simple config: SPON on first 4 layers only\n",
    "config = SPONConfig(\n",
    "    name=\"TOP-25\",\n",
    "    layer_mask=[0, 1, 2, 3],  # First 4 of 16 layers\n",
    "    modules=[\"down_proj\"]\n",
    ")\n",
    "\n",
    "# Training args (quick demo)\n",
    "args = TrainingArgs(\n",
    "    epochs=2,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=4,\n",
    "    block_size=64,  # Shorter for demo\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# Create calibration data\n",
    "print(\"Creating calibration data...\")\n",
    "dataloader = create_calibration_dataloader(\n",
    "    tokenizer,\n",
    "    block_size=64,\n",
    "    batch_size=4,\n",
    "    num_samples=128  # Small for demo\n",
    ")\n",
    "\n",
    "print(f\"Created {len(dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SPON biases\n",
    "trainer = SPONTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    sparsity=0.5,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Training {sum(p.numel() for p in trainer.spon_params):,} SPON parameters...\")\n",
    "metrics = trainer.train(dataloader)\n",
    "\n",
    "print(f\"\\nFinal loss: {metrics['final_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics['training_loss'], 'b-o')\n",
    "plt.xlabel(\"Logging Step\")\n",
    "plt.ylabel(\"KL Divergence Loss\")\n",
    "plt.title(\"SPON Training Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating SPON\n",
    "\n",
    "Let's compare perplexity with and without SPON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import compute_perplexity\n",
    "\n",
    "# Get trained biases\n",
    "spon_biases = trainer.get_spon_biases()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Evaluation dataloader (smaller)\n",
    "eval_dataloader = create_calibration_dataloader(\n",
    "    tokenizer,\n",
    "    block_size=64,\n",
    "    batch_size=4,\n",
    "    num_samples=64\n",
    ")\n",
    "\n",
    "# Dense baseline (reload model for clean state)\n",
    "print(\"Computing dense PPL...\")\n",
    "dense_result = compute_perplexity(model, eval_dataloader, device, use_sparse=False)\n",
    "print(f\"Dense PPL: {dense_result.perplexity:.2f}\")\n",
    "\n",
    "# TEAL only (no SPON)\n",
    "print(\"\\nComputing TEAL-only PPL...\")\n",
    "teal_result = compute_perplexity(\n",
    "    model, eval_dataloader, device,\n",
    "    use_sparse=True, sparsity=0.5, spon_biases=None\n",
    ")\n",
    "print(f\"TEAL PPL: {teal_result.perplexity:.2f}\")\n",
    "\n",
    "# TEAL + SPON\n",
    "print(\"\\nComputing TEAL+SPON PPL...\")\n",
    "spon_result = compute_perplexity(\n",
    "    model, eval_dataloader, device,\n",
    "    use_sparse=True, sparsity=0.5, spon_biases=spon_biases\n",
    ")\n",
    "print(f\"TEAL+SPON PPL: {spon_result.perplexity:.2f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dense (baseline):     {dense_result.perplexity:.2f}\")\n",
    "print(f\"TEAL only:            {teal_result.perplexity:.2f} (+{(teal_result.perplexity/dense_result.perplexity - 1)*100:.1f}%)\")\n",
    "print(f\"TEAL + SPON:          {spon_result.perplexity:.2f} (+{(spon_result.perplexity/dense_result.perplexity - 1)*100:.1f}%)\")\n",
    "improvement = (teal_result.perplexity - spon_result.perplexity) / teal_result.perplexity * 100\n",
    "print(f\"\\nSPON improvement:     {improvement:.1f}% PPL reduction vs TEAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "You've learned the basics! Next notebooks:\n",
    "\n",
    "1. **02_visualize_results.ipynb** - Analyze experimental results, plot Pareto frontiers\n",
    "2. **03_layer_analysis.ipynb** - Understand which layers benefit most from SPON\n",
    "\n",
    "To run the full experiment suite:\n",
    "```bash\n",
    "python experiments/run_allocation_sweep.py \\\n",
    "    --model meta-llama/Llama-3.2-1B \\\n",
    "    --configs BASELINE-TEAL UNIF-ALL TOP-25 TOP-50 \\\n",
    "    --sparsity 0.5 \\\n",
    "    --epochs 10\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
