{
  "model_name": "meta-llama/Llama-3.2-1B",
  "model_num_layers": 0,
  "model_hidden_dim": 0,
  "sparsity": 0.5,
  "config_name": "",
  "layer_mask": [],
  "modules": [],
  "epochs": 10,
  "learning_rate": 1e-05,
  "batch_size": 8,
  "block_size": 128,
  "calibration_dataset": "wikitext",
  "calibration_samples": 2048,
  "seed": 42,
  "extra": {
    "all_sparsities": [
      0.5,
      0.6
    ],
    "all_configs": [
      "BASELINE-TEAL",
      "UNIF-ALL",
      "TOP-50",
      "TOP-75",
      "BOTTOM-50",
      "ATTN-ONLY"
    ],
    "default_sparse_modules": [
      "down_proj"
    ],
    "data_driven_overrides": null,
    "model_provider_requested": "auto",
    "model_provider_effective": "huggingface",
    "ollama_model": "llama3.2:1b",
    "provider_fallback_reason": ""
  }
}